{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text with a pre-trained GPT2 in PyTorch\n",
    "\n",
    "This notebook was created as a part of a blog post - [Fine-tuning large Transformer models on a single GPU in PyTorch - Teaching GPT-2 a sense of humor](https://mf1024.github.io/2019/11/12/Fun-With-GPT-2/).\n",
    "\n",
    "In this notebook, I will use a pre-trained medium-sized GPT2 model from the [huggingface](https://github.com/huggingface/transformers) to generate some text.\n",
    "\n",
    "The easiest way to use huggingface transformer libraries is to install their pip package *transformers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from transformers) (20.9)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2021.4.4-cp36-cp36m-win_amd64.whl (269 kB)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from transformers) (3.10.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.2-cp36-cp36m-win_amd64.whl (2.0 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: six in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading click-8.0.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dev\\anaconda3\\envs\\txtgen\\lib\\site-packages (from click->sacremoses->transformers) (0.4.4)\n",
      "Installing collected packages: urllib3, idna, chardet, tqdm, requests, regex, joblib, filelock, click, tokenizers, sacremoses, numpy, huggingface-hub, dataclasses, transformers\n",
      "Successfully installed chardet-4.0.0 click-8.0.0 dataclasses-0.8 filelock-3.0.12 huggingface-hub-0.0.8 idna-2.10 joblib-1.0.1 numpy-1.19.5 regex-2021.4.4 requests-2.25.1 sacremoses-0.0.45 tokenizers-0.10.2 tqdm-4.60.0 transformers-4.6.0 urllib3-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA FOUND\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA FOUND\")\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and classes\n",
    "\n",
    "I use the [GPT2LMHeadModel](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L491) module for the language model, which is [GPT2Model](https://github.com/huggingface/transformers/blob/master/transformers/modeling_gpt2.py#L326), with an additional linear layer that uses input embedding layer weights to do the inverse operation of the embedding layer - to create logits vector for the dictionary from outputs of the GPT2.\n",
    "\n",
    "[GPT2Tokenizer](https://github.com/huggingface/transformers/blob/master/transformers/tokenization_gpt2.py#L106) is a byte-code pair encoder that will transform input text input into input tokens that the huggingface transformers were trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b358604b79034fc3ad65d6ce371814fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67d1603542a4557af0df78678cb9ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a366403052b94bab887a39548ffa6ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d5e0d45d604fed909397309cf9da7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820f1956237a4bfd920a8603df5e9cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
    "# get random token ID\n",
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "At each prediction step, GPT2 model needs to know all of the previous sequence elements to predict the next one. Below is a function that will tokenize the starting input text, and then in a loop, one new token is predicted at each step and is added to the sequence, which will be fed into the model in the next step. In the end, the token list is decoded back into a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_some_text(input_str, text_len = 250):\n",
    "\n",
    "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(text_len):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
    "\n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the text\n",
    "\n",
    "I will give thre different sentence beginnings to the GPT2 and let it generate the rest:\n",
    "\n",
    "\n",
    "***1. The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work… when you go to church… when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth…***\n",
    "\n",
    "***2. Artificial general intelligence is…***\n",
    "\n",
    "***3. The Godfather: “I’m going to make him an offer he can’t refuse.”…***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " david went to zoo. I'm sure he'll tell you that the only thing more frustrating than being stuck with your dog is the constant reminders that you have to go to the vet.\" – David, San Francisco, CA\n",
      "\n",
      "\"You've probably heard by now that I'm an animal lover and that I've tried to get my dog to live a long and happy life. But there's a catch: I've got to find a way to pay for the care that I don't want to have to take, so I'm not happy with the state of my house.\n",
      "\n",
      "So I've been considering the option of giving up the dog, and I think my dog needs to be taken care of. I've heard of people doing what I did, of dogs who've had their lives ended by neglectful owners, but I think there's one thing that I could do to prevent it.\n",
      "\n",
      "I have three cats who I love dearly. My dogs are the only two who are not in a good state. So I think that I should consider giving my cats away so that they can have a better quality of life.\n",
      "\n",
      "This could save me hundreds of dollars, and it might even be worth the hassle. I don't know if you're\n"
     ]
    }
   ],
   "source": [
    "generate_some_text(\" david went to zoo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love cooking with this dish. It's really easy to make, and tastes wonderful. If you're feeling especially adventurous or just want the perfect treat, you could also use this for a snack or a lunch. You can also make this in batches if you'd rather eat the same thing over and over again and not have it get messy.\n",
      "\n",
      "This recipe is so easy to make, you'll have a blast doing it on your lunch break. I promise, you'll be smiling and laughing the entire time you're cooking!\n",
      "\n",
      "4.8 from 5 reviews The Best Vegan Cheesecake Print Prep time 5 mins Cook time 15 mins Total time 25 mins Vegan Cheesecake made with just a handful of ingredients, it will have you craving more. Course: Dessert Serves: 6 This is one of my all-time favorite desserts so I can only imagine how delicious these will be on our dinner table. Author: Allison Recipe type: Dessert Cuisine: Vegan Serves: 6 Author: Allison Ingredients 1/2 cup coconut milk\n",
      "\n",
      "3 tablespoons almond milk (I used my regular, unrefined milk)\n",
      "\n",
      "1 cup sugar (I used Stevia)\n",
      "\n",
      "1 teaspoon vanilla\n",
      "\n",
      "2 eggs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_some_text(\"i love cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick indian snack recepie. It has no fat in it and has a sweet taste and is great served with a salad or side dish to go with it. If you like your Indian snacks to be spicy then this is the snack for you.\n",
      "\n",
      "This snack is very versatile. It can be served as a side dish as well.\n",
      "\n",
      "This is my first time making indian snacks, so I am not really sure how it is supposed to be enjoyed. But my wife likes them, and I am happy about that, so I'm sharing the recipe with you. Enjoy! (I have no idea why you would make indian snacks, but they are delicious).\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "10 cups water\n",
      "\n",
      "1/4 cup white vinegar\n",
      "\n",
      "1/2 tsp ground cumin\n",
      "\n",
      "1/4 tsp turmeric\n",
      "\n",
      "1/2 cup finely chopped coriander leaves\n",
      "\n",
      "1/2 cup sliced onions\n",
      "\n",
      "1/2 cup cilantro leaves\n",
      "\n",
      "2 cups plain Indian bread crumbs\n",
      "\n",
      "2 tsp curry powder\n",
      "\n",
      "2 tsp salt\n",
      "\n",
      "1 tsp black pepper\n",
      "\n",
      "1 cup finely chopped tomatoes\n",
      "\n",
      "Method:\n",
      "\n",
      "In a small saucepan heat water. Add 1/4 to 1/2 cup\n"
     ]
    }
   ],
   "source": [
    "generate_some_text(\"quick indian snack recepie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who killed rajendra kadak] was not in our area. We had to leave our homes in panic,\" Sohrab says.\n",
      "\n",
      "Rajendra's parents say he is in critical condition and is expected to recover soon.\n",
      "\n",
      "\"He is in a very critical condition and we have given him all kinds of drugs, including opium. We have even given him a poison to make him die,\" Rajendra's mother Soman says.\n",
      "\n",
      "\"We have taken a lot of painkillers to relieve the pain we have been suffering from our son. We are now going to the hospital to take some blood and take some pills to give him the best possible treatment,\" Sohrab adds.\n",
      "\n",
      "Rajendra's family also says their only child had an IQ of around 130.\n",
      "\n",
      "\"He was the only son of our family who worked with his hands. He was not afraid of anything. He was always doing something for the community,\" Sohrab says.\n",
      "\n",
      "Sohrab's son says his parents were in a very difficult position after a few months and they didn't know which way to go after that. They tried everything but couldn't figure out a solution. They were in desperate need of money for food and they\n"
     ]
    }
   ],
   "source": [
    "generate_some_text(\"who killed raj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
